{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ":#ðŸ“˜ GET TO KNOW A DATASET: ivrit-ai Crowd-Transcribe v5\n"
      ],
      "metadata": {
        "id": "pCKe8wY12c91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§© 1. Introduction\n",
        "\n",
        "This notebook introduces the ivrit-ai Crowd-Transcribe v5 dataset â€” a high-quality, volunteer-validated Hebrew speech transcription dataset.\n",
        "It contains:\n",
        "\n",
        "* Raw audio\n",
        "* Machine-generated original sentences\n",
        "* Human-corrected sentences\n",
        "* Metadata including segment duration, quality flags, and transcription worker IDs\n",
        "\n",
        "The dataset is stored as Parquet files on [Hugging Face](https://huggingface.co/datasets/ivrit-ai/crowd-transcribe-v5)\n",
        "\n",
        "The goal of this notebook is to help users:\n",
        "1. Explore and understand the dataset structure\n",
        "2. Inspect transcription quality\n",
        "3. Play audio segments\n",
        "4. Compute statistics (durations, worker activity, error patterns)"
      ],
      "metadata": {
        "id": "_Y4W7JSt2jpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEtzZhmz2YNN"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = \"/path/to/ivrit-ai-audio-v2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ 2. Setup"
      ],
      "metadata": {
        "id": "BVPmTRIw3c62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "from datasets import load_dataset\n",
        "\n",
        "import json\n",
        "import ast"
      ],
      "metadata": {
        "id": "zJFfmnKv3gTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the Dataset\n",
        "\n",
        "Load directly from Hugging Face:\n"
      ],
      "metadata": {
        "id": "ZIn_7PUH3iVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"ivrit-ai/crowd-transcribe-v5\")\n",
        "ds\n"
      ],
      "metadata": {
        "id": "9csooKjv3k2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f your dataset has a single split, it loads as \"train\".\n",
        "If it has multiple splits, adjust accordingly.\n",
        "\n",
        "Convert to a pandas DataFrame for convenience:"
      ],
      "metadata": {
        "id": "jfu5ew8d7cK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = ds[\"train\"].to_pandas()\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "C68fbkst7ebW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Inspect Schema"
      ],
      "metadata": {
        "id": "xVAXHysT7gZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "J2pUvxVW7ksH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected columns:\n",
        "\n",
        "* uuid - formatted as \"Tziun3/2021.06.03 ×¤×¨×§ #207 â€“ ×—×–×¨×ª×• ×©×œ ×”×™×•× ×™/343\" â†’ meaning: `<source>/<episode name>/<segment #>`\n",
        "\n",
        "* audio - an audio object\n",
        "\n",
        "* orig_sentence - machine transcription\n",
        "\n",
        "* sentence - human-corrected transcription\n",
        "\n",
        "* is_retranscribe - True if re-labeled for quality control\n",
        "\n",
        "* transcriber - numeric worker ID\n",
        "\n",
        "* extra_data - a dict with metadata"
      ],
      "metadata": {
        "id": "58891w-x7jho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Parse extra_data Into Columns\n",
        "\n",
        "The extra_data field is a dict.\n",
        "\n",
        "Extract key values into normal columns."
      ],
      "metadata": {
        "id": "DvP9ijQE3ndv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_extra(row):\n",
        "    d = row\n",
        "    if isinstance(row, str):\n",
        "        d = json.loads(row)\n",
        "    return {\n",
        "        \"duration\": d.get(\"duration\"),\n",
        "        \"foreign_language\": d.get(\"foreign_language\"),\n",
        "        \"multiple_speakers\": d.get(\"multiple_speakers\"),\n",
        "        \"noisy\": d.get(\"noisy\"),\n",
        "        \"skipped\": d.get(\"skipped\"),\n",
        "        \"max_logprob\": d.get(\"max_logprob\"),\n",
        "        \"orig_text_adapted\": d.get(\"orig_text\"),\n",
        "        \"too_long\": d.get(\"too_long\"),\n",
        "        \"unintelligible\": d.get(\"unintelligible\")\n",
        "    }\n",
        "\n",
        "extra_df = df[\"extra_data\"].apply(extract_extra).apply(pd.Series)\n",
        "df = pd.concat([df, extra_df], axis=1)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "k65mnI8C8KKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Derive Source, Episode, and Segment ID"
      ],
      "metadata": {
        "id": "9D33LaCR3rQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parts = df[\"uuid\"].str.split(\"/\", n=2, expand=True)\n",
        "df[\"source\"] = parts[0]\n",
        "df[\"episode\"] = parts[1]\n",
        "df[\"segment_id\"] = parts[2]\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "9AMkyoGZ8SfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Basic Dataset Statistics"
      ],
      "metadata": {
        "id": "vOSP0F_s8UuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Sources\n",
        "\n",
        "df[\"source\"].nunique()\n"
      ],
      "metadata": {
        "id": "NAGEoKbL3uHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segments Per Source\n",
        "\n",
        "df_source_counts = df[\"source\"].value_counts()\n",
        "df_source_counts.head()"
      ],
      "metadata": {
        "id": "qumnJ9vX31LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_counts.head(20).plot(kind=\"bar\", figsize=(12,4), title=\"Top 20 Sources by Segment Count\")\n"
      ],
      "metadata": {
        "id": "3-xOC3Vh35xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Duration Statistics"
      ],
      "metadata": {
        "id": "_tG292Mw37ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"duration\"].describe()\n",
        "\n",
        "# total hours\n",
        "total_hours = df[\"duration\"].sum() / 3600\n",
        "total_hours"
      ],
      "metadata": {
        "id": "tiwE_dX939kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"duration\"].hist(bins=50, figsize=(12,4))\n",
        "plt.title(\"Segment Duration Distribution (sec)\")\n",
        "plt.xlabel(\"seconds\")"
      ],
      "metadata": {
        "id": "7195QWHF82p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Transcription Quality Comparison"
      ],
      "metadata": {
        "id": "OprCxcTN4AcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n"
      ],
      "metadata": {
        "id": "qiz8oa8S9WjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "jiwer computes standard transcription similarity metrics:\n",
        "\n",
        "* WER - Word Error Rate\n",
        "* WIL - Word Information Lost\n",
        "* CER - Character Error Rate"
      ],
      "metadata": {
        "id": "_U2xoeH29bsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer, cer, process_words\n",
        "\n",
        "def compute_jiwer_metrics(orig, corrected):\n",
        "    try:\n",
        "        w = wer(orig, corrected)\n",
        "        c = cer(orig, corrected)\n",
        "        transform = process_words(orig, corrected)\n",
        "        mer = transform.mer\n",
        "        wil = transform.wil\n",
        "        return pd.Series({\n",
        "            \"wer\": w,\n",
        "            \"cer\": c,\n",
        "            \"wil\": wil\n",
        "        })\n",
        "    except:\n",
        "        return pd.Series({\n",
        "            \"wer\": np.nan,\n",
        "            \"cer\": np.nan,\n",
        "            \"wil\": np.nan\n",
        "        })\n",
        "\n",
        "metrics_df = df.apply(lambda r: compute_jiwer_metrics(r[\"orig_sentence\"], r[\"sentence\"]), axis=1)\n",
        "df = pd.concat([df, metrics_df], axis=1)\n",
        "\n",
        "df[[\"wer\", \"cer\", \"wil\"]].head()\n"
      ],
      "metadata": {
        "id": "bFi3cI639js8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## License\n",
        "This dataset is released under the ivrit.ai license, a modified CC-BY license permitting use for machine-learning model training while prohibiting deepfake generation and certain misuse scenarios.\n",
        "Full terms:\n",
        "https://www.ivrit.ai/en/license-faqs/\n",
        "\n",
        "## Citation\n",
        "If you use this dataset, cite:\n",
        "\n",
        "> Marmor, Yanir; Lifshitz, Yair; Snapir, Yoad; Misgav, Kinneret (2025). *Building an Accurate Open-Source Hebrew ASR System through Crowdsourcing*. Interspeech 2025.\n",
        "\n"
      ],
      "metadata": {
        "id": "ANCdhNqy4Tz0"
      }
    }
  ]
}